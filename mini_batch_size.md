### mini_batch_size에 관함.

[Although large mini-batches improve computational efficiency by providing parallelism, research shows that large mini-batches lead to networks with a poorer ability to generalise and that take longer to train. ](https://www.topbots.com/how-solve-memory-challenges-deep-learning-neural-networks-graphcore/)

- 큰 mini batch 사이즈는 연산상의 효율을 가져다줄 수 있지만, 네트워크의 일반화 능력을 저하시키고, 더 긴 학습 시간을 갖게 한다고 함.


[참고](https://stats.stackexchange.com/questions/164876/what-is-the-trade-off-between-batch-size-and-number-of-iterations-to-train-a-neu)

[전반적인 가이드](https://www.kaggle.com/code/residentmario/full-batch-mini-batch-and-online-learning/notebook)
